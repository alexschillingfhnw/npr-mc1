{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our results from different experiments. We saved our experiment results in csv files. We will load these files and evaluate our generated answers with the ground truths with the evaluation framework `ragas`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import ast\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def process_retrieved_contexts(row):\n",
    "    if isinstance(row, str):  # If it's a string representation of a list\n",
    "        parsed = ast.literal_eval(row)  # Parse the string into a list\n",
    "        return parsed if isinstance(parsed, list) else [parsed]\n",
    "    elif isinstance(row, list):  # If it's already a list\n",
    "        return row\n",
    "    else:  # If it's something else, convert to a single-item list\n",
    "        return [row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets for 2 experiments.\n",
      "Experiment: recursive_1000_chunksize_100_overlap_ada_002_results, Number of samples: 23\n",
      "Experiment: recursive_500_chunksize_50_overlap_ada_002_results, Number of samples: 23\n"
     ]
    }
   ],
   "source": [
    "# load all csv files from ../data/experiments\n",
    "folder_path = \"../data/experiments\"\n",
    "\n",
    "# Initialize a dictionary to store dataframes for each file\n",
    "experiment_dataframes = {}\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Select columns\n",
    "        df = df[['question', 'answer', 'contexts', 'ground_truth']]\n",
    "\n",
    "        # Rename columns\n",
    "        df = df.rename(columns={\n",
    "            'question': 'user_input',\n",
    "            'contexts': 'retrieved_contexts',\n",
    "            'answer': 'response', \n",
    "            'ground_truth': 'reference'\n",
    "        })\n",
    "\n",
    "        # Ensure every row in `retrieved_contexts` is a list\n",
    "        df['retrieved_contexts'] = df['retrieved_contexts'].apply(process_retrieved_contexts)\n",
    "\n",
    "        # Convert the dataframe to a RAGAS-compatible dataset\n",
    "        dataset = df\n",
    "\n",
    "        # Save each dataset in the dictionary\n",
    "        experiment_name = os.path.splitext(file_name)[0]\n",
    "        experiment_dataframes[experiment_name] = dataset\n",
    "\n",
    "# Check loaded datasets\n",
    "print(f\"Loaded datasets for {len(experiment_dataframes)} experiments.\")\n",
    "\n",
    "for experiment_name, dataset in experiment_dataframes.items():\n",
    "    print(f\"Experiment: {experiment_name}, Number of samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "my_run_config = RunConfig(max_workers=1, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing experiment: recursive_1000_chunksize_100_overlap_ada_002_results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aedf4820d7409f8bc650b143537cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[64]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[66]: TimeoutError()\n",
      "Exception raised in Job[67]: TimeoutError()\n",
      "Exception raised in Job[68]: TimeoutError()\n",
      "Exception raised in Job[69]: TimeoutError()\n",
      "Exception raised in Job[70]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[72]: TimeoutError()\n",
      "Exception raised in Job[73]: TimeoutError()\n",
      "Exception raised in Job[74]: TimeoutError()\n",
      "Exception raised in Job[75]: TimeoutError()\n",
      "Exception raised in Job[76]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[79]: TimeoutError()\n",
      "Exception raised in Job[80]: TimeoutError()\n",
      "Exception raised in Job[81]: TimeoutError()\n",
      "Exception raised in Job[82]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[84]: TimeoutError()\n",
      "Exception raised in Job[85]: TimeoutError()\n",
      "Exception raised in Job[78]: TimeoutError()\n",
      "Exception raised in Job[86]: TimeoutError()\n",
      "Exception raised in Job[87]: TimeoutError()\n",
      "Exception raised in Job[88]: TimeoutError()\n",
      "Exception raised in Job[90]: TimeoutError()\n",
      "Exception raised in Job[91]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     samples\u001b[39m.\u001b[39mappend(sample)\n\u001b[1;32m     21\u001b[0m evaluation_dataset \u001b[39m=\u001b[39m EvaluationDataset(samples\u001b[39m=\u001b[39msamples)\n\u001b[0;32m---> 23\u001b[0m results \u001b[39m=\u001b[39m evaluate(\n\u001b[1;32m     24\u001b[0m     dataset\u001b[39m=\u001b[39mevaluation_dataset,\n\u001b[1;32m     25\u001b[0m     metrics\u001b[39m=\u001b[39mmetrics,\n\u001b[1;32m     26\u001b[0m     run_config\u001b[39m=\u001b[39mmy_run_config,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m evaluation_results[experiment_name] \u001b[39m=\u001b[39m results\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompleted evaluation for \u001b[39m\u001b[39m{\u001b[39;00mexperiment_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/_analytics.py:130\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m    129\u001b[0m     track(IsCompleteEvent(event_type\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, is_completed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[0;32m--> 130\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    131\u001b[0m     track(IsCompleteEvent(event_type\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, is_completed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/evaluation.py:303\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size)\u001b[0m\n\u001b[1;32m    300\u001b[0m scores: t\u001b[39m.\u001b[39mList[t\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, t\u001b[39m.\u001b[39mAny]] \u001b[39m=\u001b[39m []\n\u001b[1;32m    301\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[39m# get the results\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     results \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mresults()\n\u001b[1;32m    304\u001b[0m     \u001b[39mif\u001b[39;00m results \u001b[39m==\u001b[39m []:\n\u001b[1;32m    305\u001b[0m         \u001b[39mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/executor.py:200\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m             nest_asyncio\u001b[39m.\u001b[39mapply()\n\u001b[1;32m    198\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nest_asyncio_applied \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m results \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_jobs())\n\u001b[1;32m    201\u001b[0m sorted_results \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(results, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m])\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m [r[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m sorted_results]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     31\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_once()\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[39m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m ready \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_when \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime(), \u001b[39m0\u001b[39m), \u001b[39m86400\u001b[39m) \u001b[39mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mselect(timeout)\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/selectors.py:561\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    560\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     kev_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mcontrol(\u001b[39mNone\u001b[39;00m, max_ev, timeout)\n\u001b[1;32m    562\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset, SingleTurnSample\n",
    "\n",
    "# Define metrics to evaluate\n",
    "metrics = [context_precision, context_recall, faithfulness, answer_relevancy]\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for experiment_name, eval_dataset in experiment_dataframes.items():\n",
    "    print(f\"Processing experiment: {experiment_name}\")\n",
    "    \n",
    "    samples = []\n",
    "    for _, row in eval_dataset.iterrows():\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=row['user_input'],\n",
    "            reference=row['reference'],\n",
    "            response=row['response'],\n",
    "            retrieved_contexts=row['retrieved_contexts']\n",
    "        )\n",
    "        samples.append(sample)\n",
    "    \n",
    "    evaluation_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "    results = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=metrics,\n",
    "        run_config=my_run_config,\n",
    "    )\n",
    "    evaluation_results[experiment_name] = results\n",
    "    print(f\"Completed evaluation for {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd9c78cbb62892232a2e8cf9a4bd699d988202e949c50bb9be5232199c394801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
