{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load different chunking strategies and their embeddings with the models text-embedding-ada-002 and text-embedding-large-3. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "\n",
    "import credentials\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from pinecone import Pinecone, Index, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name='gpt-4' #This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickled data\n",
    "with open('../embeddings/recursive_1000_chunksize_100_overlap_ada_002_embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing embeddings, caused by empty text chunks, API errors, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid embeddings: 44545\n",
      "Total invalid embeddings (None): 250\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with None embeddings\n",
    "valid_embeddings = embeddings[embeddings['embeddings'].notna()]\n",
    "\n",
    "# Debug: Check how many rows remain\n",
    "print(f\"Total valid embeddings: {len(valid_embeddings)}\")\n",
    "print(f\"Total invalid embeddings (None): {len(embeddings) - len(valid_embeddings)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embeddings in a Vector Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Pinecone Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone with the new class-based approach\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "\n",
    "# Define the index name and check if it exists\n",
    "index_name = \"npr-mc1-new\"\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=len(embeddings['embeddings'][0]),  # Dimension should match embedding model's output dimension\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store Embeddings in Pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for idx, row in valid_embeddings.iterrows():\n",
    "    doc_id = str(row['index'] if 'index' in valid_embeddings.columns else idx)\n",
    "    embedding = row['embeddings']\n",
    "    original_text = row['content_chunks']\n",
    "\n",
    "    # Validate embedding is a list of floats\n",
    "    if isinstance(embedding, list) and all(isinstance(value, float) for value in embedding):\n",
    "        records.append({\n",
    "            \"id\": doc_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\"text\": original_text}\n",
    "        })\n",
    "\n",
    "# Upsert valid records in batches\n",
    "BATCH_SIZE = 100\n",
    "for i in range(0, len(records), BATCH_SIZE):\n",
    "    batch = records[i:i + BATCH_SIZE]\n",
    "    try:\n",
    "        pinecone_index.upsert(vectors=batch)\n",
    "        \n",
    "        # Print progress every 50 batches\n",
    "        if (i // BATCH_SIZE + 1) % 50 == 0:\n",
    "            print(f\"Upserted batch {i // BATCH_SIZE + 1}/{(len(records) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting batch {i // BATCH_SIZE + 1}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing (Retriever Module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a retrieval function that uses Pinecone for fetching relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, embedding_model):\n",
    "    # Generate embeddings for a given text using the specified embedding model.\n",
    "    response = client.embeddings.create(input=[text], model=embedding_model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def retrieve_relevant_chunks(query, embedding_model, top_k=5):\n",
    "    # Embed the query using the embedding function\n",
    "    query_embedding = generate_embeddings(query, embedding_model)\n",
    "\n",
    "    # Retrieve similar documents from Pinecone\n",
    "    results = pinecone_index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Query and Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: 1\n",
      "eyeing a phased expansion to 126 million tons/yr. QP says it should be able to eliminate routine gas flaring by 2030, with methane emissions limited by setting a methane intensity target of 02% across all facilities by 2025. The company also plans to build some 16 gigawatts of solar energy capacity ...\n",
      "Similarity Score: 0.89\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 0\n",
      "Qatar Petroleum (QP) is targeting aggressive cuts in its greenhouse gas emissions as it prepares to launch Phase 2 of its planned 48 million ton per year LNG expansion. In its latest Sustainability Report published on Wednesday, QP said its goals include reducing the emissions intensity of Qatar's L...\n",
      "Similarity Score: 0.87\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 24499\n",
      "buyers have emerged, pressure is growing to reduce the environmental footprint of the global gas trade. With plans to remain dominant in the sector as it has been for decades, QP has been making moves to align with the global climate push. The company signed its first long term deal late last year t...\n",
      "Similarity Score: 0.86\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 24497\n",
      "State owned Qatar Petroleum (QP), the worlds leading liquefied natural gas (LNG) exporter, has unveiled an ambitious plan to cut emissions and drastically reduce the environmental footprint of its fossil fuel operations. The company said its sustainability strategy establishes targets to align with ...\n",
      "Similarity Score: 0.86\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 2\n",
      "estimates to be worth around $ 35 billion, is expected to be awarded by Mar 31. Shortly after the construction contract is awarded, QP is expected to select foreign investments partners to take stakes of up to 30% in the Phase 1 trains. Exxon Mobil, Royal Dutch Shell, Total, Chevron, ConocoPhillips ...\n",
      "Similarity Score: 0.86\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Qatar Petroleum mention what will happen in Phase 1 of the LNG expansion?\"\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "relevant_chunks = retrieve_relevant_chunks(query, embedding_model, top_k=5)\n",
    "\n",
    "# Loop through each retrieved chunk and print its details\n",
    "for match in relevant_chunks.matches:\n",
    "    chunk_id = match.id\n",
    "    text_content = match.metadata.get('text', '')[:300]\n",
    "    score = match.score\n",
    "    \n",
    "    print(f\"Chunk ID: {chunk_id}\")\n",
    "    print(f\"{text_content}...\")\n",
    "    print(f\"Similarity Score: {score:.2f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers (Generator Module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the Generator Module to generate answers based on the retrieved chunks using Azure OpenAI's GPT-4. In this step, we will create a function that takes a user query and the retrieved chunks, composes a relevant context from those chunks, and then uses GPT-4 to generate an answer based on this context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`model`**: Specifies the model, in our case GPT-4, for generating responses.\n",
    "- **`max_tokens`**: Limits response length.\n",
    "- **`temperature`**: Controls randomness; lower = focused, higher = creative.\n",
    "- **`top_p`**: Limits choices to most likely words for coherent output.\n",
    "- **`top_k`**: Restricts to top-k choices, narrowing token selection.\n",
    "- **`stop`**: Defines where the model should stop generating for clean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a highly knowledgeable AI assistant specializing in providing accurate and contextually relevant answers. \n",
    "    Use the context provided below to answer the user's query as thoroughly and concisely as possible. \n",
    "    If the context does not contain sufficient information to answer the query, say so explicitly. \n",
    "    Do not include any information not supported by the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, chunks):\n",
    "    \"\"\"\n",
    "    Generate an answer to the given query using retrieved chunks and Azure OpenAI GPT-4.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        chunks (list): Retrieved chunks containing context.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    # Compose the context from the retrieved chunks, handling potential missing metadata\n",
    "    context = \" \".join(chunk.get('metadata', {}).get('text', '') for chunk in chunks['matches'])\n",
    "\n",
    "    # Ensure the context isn't empty\n",
    "    if not context.strip():\n",
    "        return \"The provided context does not contain sufficient information to answer the query.\"\n",
    "   \n",
    "    # Generate the answer using Azure OpenAI GPT-4\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        model = deployment_name,\n",
    "        max_tokens=150,\n",
    "        temperature=0.1,            # Lower temperature for concise and deterministic answers\n",
    "        stop=[\"End of answer\"],     # Optional stop sequence for clean output\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " Qatar Petroleum (QP) mentioned that Phase 1 of the LNG expansion, also known as the North Field East project, will contribute to the company's carbon capture goal by capturing about 22 million tons/yr of carbon. This phase will increase Qatar's LNG production capacity by 32 million tons/yr. However, bids for the construction of all four trains for Phase 1 were deemed too expensive and none met QP's targeted 50-week construction schedule. Contractors were asked to submit new bids with cost savings. Once the construction contract is awarded, QP is expected to select foreign investment partners to take stakes of up to 30% in the Phase 1 trains. Exxon Mobil, Royal Dutch Shell, Total, Chevron, ConocoPhillips, and Eni have been shortlisted for this.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer based on retrieved chunks\n",
    "generated_answer = generate_answer(query, relevant_chunks)\n",
    "print(\"Generated Answer:\\n\", generated_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLEU Score: Measures the overlap of n-grams between the generated answer and the reference answer. This metric is valuable for measuring content similarity, especially for factual information.\n",
    "- ROUGE Score: Commonly used for summarization tasks, it also evaluates the overlap of n-grams but considers recall more heavily, which is beneficial for checking if generated responses capture the core content.\n",
    "- Cosine Similarity: Measures the semantic similarity between the generated answer and the reference answer in embedding space. This ensures that even if the wording differs, the underlying meaning is still preserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**:\n",
    "\n",
    "We have `data_eval`, which contains the following fields `example_id`, `question_id`, `question`, `relevant_text`, `answer` and `article_url`. Each row represents an evaluation example, with the `question` to be queried in our pipeline, the `relevant_text` providing context for manual verification, and `answer` as the ground-truth answer to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevant_text</th>\n",
       "      <th>answer</th>\n",
       "      <th>article_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the innovation behind Leclanché's new ...</td>\n",
       "      <td>Leclanché said it has developed an environment...</td>\n",
       "      <td>Leclanché's innovation is using a water-based ...</td>\n",
       "      <td>https://www.sgvoice.net/strategy/technology/23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>The Green Deal Industrial Plan is a bid by the...</td>\n",
       "      <td>The EU’s Green Deal Industrial Plan aims to en...</td>\n",
       "      <td>https://www.sgvoice.net/policy/25396/eu-seeks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>The European counterpart to the US Inflation R...</td>\n",
       "      <td>The EU’s Green Deal Industrial Plan aims to en...</td>\n",
       "      <td>https://www.pv-magazine.com/2023/02/02/europea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>What are the four focus areas of the EU's Gree...</td>\n",
       "      <td>The new plan is fundamentally focused on four ...</td>\n",
       "      <td>The four focus areas of the EU's Green Deal In...</td>\n",
       "      <td>https://www.sgvoice.net/policy/25396/eu-seeks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>When did the cooperation between GM and Honda ...</td>\n",
       "      <td>What caught our eye was a new hookup between G...</td>\n",
       "      <td>July 2013</td>\n",
       "      <td>https://cleantechnica.com/2023/05/08/general-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id  question_id                                           question  \\\n",
       "0           1            1  What is the innovation behind Leclanché's new ...   \n",
       "1           2            2       What is the EU’s Green Deal Industrial Plan?   \n",
       "2           3            2       What is the EU’s Green Deal Industrial Plan?   \n",
       "3           4            3  What are the four focus areas of the EU's Gree...   \n",
       "4           5            4  When did the cooperation between GM and Honda ...   \n",
       "\n",
       "                                       relevant_text  \\\n",
       "0  Leclanché said it has developed an environment...   \n",
       "1  The Green Deal Industrial Plan is a bid by the...   \n",
       "2  The European counterpart to the US Inflation R...   \n",
       "3  The new plan is fundamentally focused on four ...   \n",
       "4  What caught our eye was a new hookup between G...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Leclanché's innovation is using a water-based ...   \n",
       "1  The EU’s Green Deal Industrial Plan aims to en...   \n",
       "2  The EU’s Green Deal Industrial Plan aims to en...   \n",
       "3  The four focus areas of the EU's Green Deal In...   \n",
       "4                                          July 2013   \n",
       "\n",
       "                                         article_url  \n",
       "0  https://www.sgvoice.net/strategy/technology/23...  \n",
       "1  https://www.sgvoice.net/policy/25396/eu-seeks-...  \n",
       "2  https://www.pv-magazine.com/2023/02/02/europea...  \n",
       "3  https://www.sgvoice.net/policy/25396/eu-seeks-...  \n",
       "4  https://cleantechnica.com/2023/05/08/general-m...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eval = pd.read_csv('../data/evaluation/cleantech_rag_evaluation_data_2024-09-20.csv', delimiter=';')\n",
    "data_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query, embedding_model, top_k=5):\n",
    "        return self.retriever(query, embedding_model, top_k)\n",
    "\n",
    "    def generate_answer(self, query, retrieved_chunks):\n",
    "        return self.generator(query, retrieved_chunks)\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = RAGPipeline(\n",
    "    retriever=retrieve_relevant_chunks,\n",
    "    generator=generate_answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_bleu_scores(reference, hypothesis):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    scores = {}\n",
    "    for n in range(1, 5):  # BLEU-1 to BLEU-4\n",
    "        weights = tuple((1 / n) for _ in range(n)) + (0,) * (4 - n)\n",
    "        scores[f\"bleu_{n}\"] = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            hypothesis.split(),\n",
    "            weights=weights,\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "def calculate_rouge_scores(reference, hypothesis):\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    rouge_scores = rouge.score(reference, hypothesis)\n",
    "    rouge1 = rouge_scores['rouge1'].fmeasure\n",
    "    rouge2 = rouge_scores['rouge2'].fmeasure\n",
    "    rougeL = rouge_scores['rougeL'].fmeasure\n",
    "    return {\"rouge1\": rouge1, \"rouge2\": rouge2, \"rougeL\": rougeL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating RAG Pipeline: 100%|██████████| 23/23 [01:31<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate evaluation metrics for each example\n",
    "def evaluate_example(query, true_answer, pipeline, embedding_model):\n",
    "\n",
    "    relevant_chunks = pipeline.retrieve_relevant_chunks(query, embedding_model, top_k=5)\n",
    "    generated_answer = pipeline.generate_answer(query, relevant_chunks)\n",
    "\n",
    "    # Cosine Similarity\n",
    "    true_embedding = np.array(generate_embeddings(true_answer, embedding_model)).reshape(1, -1)\n",
    "    generated_embedding = np.array(generate_embeddings(generated_answer, embedding_model)).reshape(1, -1)\n",
    "    cosine_sim = cosine_similarity(true_embedding, generated_embedding)[0][0]\n",
    "\n",
    "    # BLEU and ROUGE Scores\n",
    "    bleu_scores = calculate_bleu_scores(true_answer, generated_answer)\n",
    "    rouge_scores = calculate_rouge_scores(true_answer, generated_answer)\n",
    "\n",
    "    # relevant chunks to list\n",
    "    relevant_chunks_list = [chunk.get('metadata', {}).get('text', '') for chunk in relevant_chunks['matches']]\n",
    "\n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"ground_truth\": true_answer,\n",
    "        \"answer\": generated_answer,\n",
    "        \"contexts\": relevant_chunks_list,\n",
    "        **bleu_scores,\n",
    "        **rouge_scores,\n",
    "        \"cosine_similarity\": cosine_sim\n",
    "    }\n",
    "\n",
    "# Function to evaluate the entire dataset\n",
    "def evaluate_pipeline(data_eval, pipeline, embedding_model):\n",
    "    results = []\n",
    "    for _, row in tqdm(data_eval.iterrows(), total=len(data_eval), desc=\"Evaluating RAG Pipeline\"):\n",
    "        query = row['question']\n",
    "        true_answer = row['answer']\n",
    "        \n",
    "        # Evaluate this example\n",
    "        example_results = evaluate_example(query, true_answer, pipeline, embedding_model)\n",
    "        results.append(example_results)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "evaluation_results = evaluate_pipeline(data_eval, pipeline, embedding_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the innovation behind Leclanché's new ...</td>\n",
       "      <td>Leclanché has developed an environmentally fri...</td>\n",
       "      <td>[The new battery leverages highly conductive b...</td>\n",
       "      <td>Leclanché's innovation is using a water-based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>The EU's Green Deal Industrial Plan is an init...</td>\n",
       "      <td>['The EU has presented its Green Deal Industri...</td>\n",
       "      <td>The EU’s Green Deal Industrial Plan aims to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>The EU's Green Deal Industrial Plan is a strat...</td>\n",
       "      <td>['The EU has presented its Green Deal Industri...</td>\n",
       "      <td>The EU’s Green Deal Industrial Plan aims to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the four focus areas of the EU's Gree...</td>\n",
       "      <td>The four focus areas, or pillars, of the EU's ...</td>\n",
       "      <td>['The EU has presented its Green Deal Industri...</td>\n",
       "      <td>The four focus areas of the EU's Green Deal In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did the cooperation between GM and Honda ...</td>\n",
       "      <td>The cooperation between GM and Honda on fuel c...</td>\n",
       "      <td>[collaboration launched in July of 2013, provi...</td>\n",
       "      <td>July 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the innovation behind Leclanché's new ...   \n",
       "1       What is the EU’s Green Deal Industrial Plan?   \n",
       "2       What is the EU’s Green Deal Industrial Plan?   \n",
       "3  What are the four focus areas of the EU's Gree...   \n",
       "4  When did the cooperation between GM and Honda ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Leclanché has developed an environmentally fri...   \n",
       "1  The EU's Green Deal Industrial Plan is an init...   \n",
       "2  The EU's Green Deal Industrial Plan is a strat...   \n",
       "3  The four focus areas, or pillars, of the EU's ...   \n",
       "4  The cooperation between GM and Honda on fuel c...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [The new battery leverages highly conductive b...   \n",
       "1  ['The EU has presented its Green Deal Industri...   \n",
       "2  ['The EU has presented its Green Deal Industri...   \n",
       "3  ['The EU has presented its Green Deal Industri...   \n",
       "4  [collaboration launched in July of 2013, provi...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Leclanché's innovation is using a water-based ...  \n",
       "1  The EU’s Green Deal Industrial Plan aims to en...  \n",
       "2  The EU’s Green Deal Industrial Plan aims to en...  \n",
       "3  The four focus areas of the EU's Green Deal In...  \n",
       "4                                          July 2013  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = evaluation_results[[\"question\", \"answer\", \"contexts\", \"ground_truth\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric [context_precision] that is used requires the following additional columns ['reference', 'retrieved_contexts', 'user_input'] to be present in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mAZURE_OPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m ragas_dataset \u001b[39m=\u001b[39m EvaluationDataset\u001b[39m.\u001b[39mfrom_pandas(data_eval)\n\u001b[0;32m---> 13\u001b[0m result \u001b[39m=\u001b[39m evaluate(\n\u001b[1;32m     14\u001b[0m     dataset \u001b[39m=\u001b[39m ragas_dataset, \n\u001b[1;32m     15\u001b[0m     metrics\u001b[39m=\u001b[39m[\n\u001b[1;32m     16\u001b[0m         context_precision,\n\u001b[1;32m     17\u001b[0m         context_recall,\n\u001b[1;32m     18\u001b[0m         faithfulness,\n\u001b[1;32m     19\u001b[0m         answer_relevancy,\n\u001b[1;32m     20\u001b[0m     ],\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m df \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/_analytics.py:130\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m    129\u001b[0m     track(IsCompleteEvent(event_type\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, is_completed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[0;32m--> 130\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    131\u001b[0m     track(IsCompleteEvent(event_type\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, is_completed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/evaluation.py:179\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size)\u001b[0m\n\u001b[1;32m    176\u001b[0m     dataset \u001b[39m=\u001b[39m EvaluationDataset\u001b[39m.\u001b[39mfrom_list(dataset\u001b[39m.\u001b[39mto_list())\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[0;32m--> 179\u001b[0m     validate_required_columns(dataset, metrics)\n\u001b[1;32m    180\u001b[0m     validate_supported_metrics(dataset, metrics)\n\u001b[1;32m    182\u001b[0m \u001b[39m# set the llm and embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/validation.py:63\u001b[0m, in \u001b[0;36mvalidate_required_columns\u001b[0;34m(ds, metrics)\u001b[0m\n\u001b[1;32m     61\u001b[0m available_columns \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(ds\u001b[39m.\u001b[39mfeatures())\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_columns\u001b[39m.\u001b[39missubset(available_columns):\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe metric [\u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m] that is used requires the following \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madditional columns \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(required_columns\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mavailable_columns)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be present in the dataset.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The metric [context_precision] that is used requires the following additional columns ['reference', 'retrieved_contexts', 'user_input'] to be present in the dataset."
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "ragas_dataset = EvaluationDataset.from_pandas(data_eval)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = ragas_dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd9c78cbb62892232a2e8cf9a4bd699d988202e949c50bb9be5232199c394801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
