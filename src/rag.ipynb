{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load different chunking strategies and their embeddings with the models text-embedding-ada-002 and text-embedding-large-3. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "\n",
    "import credentials\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from pinecone import Pinecone, Index, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name='gpt-4'\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can load precomputed embeddings stored as pickle files. We created these embeddigs in a previous notebook (`chunking_and_embeddings.ipynb`) using different chunking strategies and embedding models (`text-embedding-ada-002` and `text-embedding-large-3`). This ensures consistency across experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific notebook we will load and process the embeddings with recursive character chunking with 1000 chunk size, 0 overlap and the text-embedding-ada-002 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from a pickle file\n",
    "with open('../embeddings/recursive_500_chunksize_50_overlap_text-embedding-3-large.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging:** We validate the loaded embeddings by checking for missing or invalid entries. This step ensures we handle API errors or processing failures that may have occurred during embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid embeddings: 108312\n",
      "Total invalid embeddings (None): 0\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with None embeddings\n",
    "valid_embeddings = embeddings[embeddings['embeddings'].notna()]\n",
    "\n",
    "# Debug: Check how many rows remain\n",
    "print(f\"Total valid embeddings: {len(valid_embeddings)}\")\n",
    "print(f\"Total invalid embeddings (None): {len(embeddings) - len(valid_embeddings)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove invalid embeddings (rows with None values) to ensure only complete data is processed further. This helps maintain the quality of our pipeline and avoids errors downstream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Setup and Embedding Management"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Pinecone Vector Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pinecone to store and manage our embeddings for retrieval tasks. Pinecone provides a scalable and fast vector database optimized for similarity search.\n",
    "\n",
    "- Setup Pinecone: We initialize the Pinecone client and check if the required index exists.\n",
    "- Define Index: If the index does not exist, we create it with the appropriate dimensions to match the embedding model. This ensures compatibility with the stored embeddings.\n",
    "- Connect to Index: If the index already exists, we directly connect to it, avoiding redundant creation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_pinecone_index(api_key, index_name, dimension=None, create_if_not_exists=True, cloud=\"aws\", region=\"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Initializes or connects to a Pinecone index.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): API key for Pinecone.\n",
    "        index_name (str): The name of the index to connect to or create.\n",
    "        dimension (int, optional): The dimension of the index embeddings (required if creating an index).\n",
    "        create_if_not_exists (bool): Whether to create the index if it does not exist. Defaults to True.\n",
    "        cloud (str): Cloud provider for the index. Defaults to \"aws\".\n",
    "        region (str): Region for the index. Defaults to \"us-east-1\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Index, bool) where Index is a connected Pinecone Index object, and bool indicates if the index is newly created.\n",
    "    \"\"\"\n",
    "    # Initialize Pinecone client\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "    is_new_index = False\n",
    "\n",
    "    if index_name not in existing_indexes:\n",
    "        if create_if_not_exists:\n",
    "            if dimension is None:\n",
    "                raise ValueError(\"Dimension must be specified when creating a new index.\")\n",
    "            # Create the index\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region)\n",
    "            )\n",
    "            print(f\"Index '{index_name}' created.\")\n",
    "            is_new_index = True\n",
    "        else:\n",
    "            raise ValueError(f\"Index '{index_name}' does not exist and create_if_not_exists is set to False.\")\n",
    "    else:\n",
    "        print(f\"Connecting to existing index '{index_name}'.\")\n",
    "\n",
    "    # Connect to the index\n",
    "    return pc.Index(index_name), is_new_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upserting Embeddings into Pinecone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifying the index status, we decide whether to proceed with the embedding upsertion process:\n",
    "\n",
    "- Embedding Preparation: We filter valid embeddings and prepare them for upsertion. Each record includes:\n",
    "    - ID: A unique identifier for the chunk\n",
    "\n",
    "    - Values: The embedding vector\n",
    "    \n",
    "    - Metadata: The original text content for reference and contextual retrieval\n",
    "\n",
    "- Conditional Upsertion: If the index is newly created, embeddings are upserted in batches. This prevents unnecessary upsertion for existing indexes.\n",
    "\n",
    "- Batching and Progress Logging: To handle large datasets efficiently and avoid rate-limit issues and help manage memory usage, embeddings are upserted in defined batch sizes, with periodic progress logs to track the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_embeddings(pinecone_index, embeddings, is_new_index, batch_size=100):\n",
    "    \"\"\"\n",
    "    Upserts embeddings into the Pinecone index if it was newly created.\n",
    "\n",
    "    Args:\n",
    "        pinecone_index (Index): Connected Pinecone Index object.\n",
    "        embeddings (pd.DataFrame): DataFrame containing embeddings and metadata.\n",
    "        is_new_index (bool): Whether the index was newly created.\n",
    "        batch_size (int): Number of records per upsert batch.\n",
    "    \"\"\"\n",
    "    if not is_new_index:\n",
    "        print(\"Index already exists. Skipping upsert.\")\n",
    "        return\n",
    "\n",
    "    # Prepare records for upserting\n",
    "    records = []\n",
    "    for idx, row in embeddings.iterrows():\n",
    "        doc_id = str(row['index'] if 'index' in embeddings.columns else idx)\n",
    "        embedding = row['embeddings']\n",
    "        original_text = row['content_chunks']\n",
    "\n",
    "        # Validate embedding is a list of floats\n",
    "        if isinstance(embedding, list) and all(isinstance(value, float) for value in embedding):\n",
    "            records.append({\n",
    "                \"id\": doc_id,\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": {\"text\": original_text}\n",
    "            })\n",
    "\n",
    "    # Upsert records in batches\n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        try:\n",
    "            pinecone_index.upsert(vectors=batch)\n",
    "            # Print progress every 50 batches\n",
    "            if (i // batch_size + 1) % 50 == 0:\n",
    "                print(f\"Upserted batch {i // batch_size + 1}/{(len(records) + batch_size - 1) // batch_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting batch {i // batch_size + 1}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach optimizes resource utilization and maintains the integrity of the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to existing index '500-chunksize-50-overlap-new-2'.\n",
      "Index already exists. Skipping upsert.\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "index_name = \"500-chunksize-50-overlap-new-2\"\n",
    "dimension = 1536  # Dimension of the embeddings\n",
    "valid_embeddings = embeddings[embeddings['embeddings'].notna()]  # Filter valid embeddings\n",
    "\n",
    "pinecone_index, is_new_index = initialize_pinecone_index(\n",
    "    api_key=pinecone_api_key,\n",
    "    index_name=index_name,\n",
    "    dimension=dimension,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "upsert_embeddings(\n",
    "    pinecone_index=pinecone_index,\n",
    "    embeddings=valid_embeddings,\n",
    "    is_new_index=is_new_index,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing (Retriever Module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Query Processing module is responsible for embedding user queries, searching the vector database (Pinecone) for the most relevant chunks of context and returning the top-k results. This module forms the backbone of our RAG chain by ensuring that the generator module has high-quality, contextually relevant information.\n",
    "\n",
    "`generate_embeddings`:\n",
    "- Generates an embedding vector for a given query using the specified embedding model (same model as the text embedding)\n",
    "- This vector is used for similarity search in the vector database\n",
    "\n",
    "`retrieve_relevant_chunks`:\n",
    "- Accepts a user query, embeds it using the generate_embeddings function and queries the Pinecone vector database to retrieve the most similar chunks.\n",
    "- Returns a ranked list of chunks based on similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, embedding_model):\n",
    "    # Generate embeddings for a given text using the specified embedding model.\n",
    "    response = client.embeddings.create(input=[text], model=embedding_model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def rerank_results(retrieved_chunks, additional_criteria=None):\n",
    "    \"\"\"\n",
    "    Reranks the retrieved chunks based on additional criteria.\n",
    "\n",
    "    Args:\n",
    "        retrieved_chunks (list): List of retrieved chunks containing metadata and similarity scores.\n",
    "        additional_criteria (function, optional): Function that can be used to compute additional scores for reranking.\n",
    "\n",
    "    Returns:\n",
    "        list: Reranked list of retrieved chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def custom_ranking(chunk):\n",
    "        relevance_score = chunk.metadata.get('relevance_score', 0)  # Default to 0 if no relevance score is available\n",
    "        return chunk.score + relevance_score  # Combine the scores to create a new ranking score\n",
    "\n",
    "    # Sort by the custom ranking score in descending order\n",
    "    reranked_chunks = sorted(retrieved_chunks.matches, key=custom_ranking, reverse=True)\n",
    "    return reranked_chunks\n",
    "\n",
    "def retrieve_relevant_chunks(query, embedding_model, top_k=5):\n",
    "    # Embed the query using the embedding function\n",
    "    query_embedding = generate_embeddings(query, embedding_model)\n",
    "\n",
    "    # Retrieve similar documents from Pinecone\n",
    "    results = pinecone_index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Rerank the results based on custom criteria\n",
    "    reranked_results = rerank_results(results, additional_criteria=None)\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current reranking strategy is relatively simple but provides a good starting point for improving the relevance of the retrieved documents. By combining similarity scores with a custom relevance score, the reranking process adjusts the order of results to ensure the most contextually relevant documents are presented first.\n",
    "\n",
    "If more time and resources were available, the reranking strategy could be significantly enhanced by applying more sophisticated methods such as Learning to Rank, user intent analysis, temporal relevance, or feedback-based adjustments. These strategies would allow for even more personalized and accurate search results, ultimately improving the overall user experience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Query and Retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates how to use the `retrieve_relevant_chunks` function to retrieve context for a given query. We defined our own question to test our RAG chain up to this point and used it as a query to retrieve relevant chunks from the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: 60015\n",
      "would detail emissions generated to produce and deliver the cargoes. The cargoes delivered would not be offset with carbon emissions credits, but they would include certificates detailing the amount of emissions released between the wellhead and import terminal.', 'QP now is working to increase LNG ...\n",
      "Similarity Score: 0.71\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 3\n",
      "the Siraj solar power project next year ( EIF Jan.22'20). Until this month, there had been little news about Phase 2 of Qatar's massive LNG expansion. But McDermott International said last week that it had been awarded the front-end engineering and design contract for five offshore wellhead platform...\n",
      "Similarity Score: 0.71\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 1\n",
      "facilities by more than 75% and has raised its carbon capture and storage ambitions from 5 million tons/yr to 7 million tons/yr by 2027. About 2.2 million tons/yr of the carbon capture goal will come from the 32 million ton/yr Phase 1 of the LNG expansion, also known as the North Field East project....\n",
      "Similarity Score: 0.68\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 0\n",
      "[\"Qatar Petroleum ( QP) is targeting aggressive cuts in its greenhouse gas emissions as it prepares to launch Phase 2 of its planned 48 million ton per year LNG expansion. In its latest Sustainability Report published on Wednesday, QP said its goals include `` reducing the emissions intensity of Qat...\n",
      "Similarity Score: 0.66\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk ID: 2\n",
      "16 million tons/yr. Qatar currently has an LNG production capacity of around 78 million tons/yr and is eyeing a phased expansion to 126 million tons/yr. QP says it should be able to eliminate routine gas flaring by 2030, with methane emissions limited `` by setting a methane intensity target of 0.2%...\n",
      "Similarity Score: 0.64\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Qatar Petroleum mention what will happen in Phase 1 of the LNG expansion?\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "\n",
    "# Retrieve and rerank the results\n",
    "relevant_chunks = retrieve_relevant_chunks(query, embedding_model, top_k=5)\n",
    "\n",
    "# Loop through each reranked chunk and print its details\n",
    "for match in relevant_chunks:\n",
    "    chunk_id = match.id\n",
    "    text_content = match.metadata.get('text', '')[:300]\n",
    "    score = match.score\n",
    "\n",
    "    print(f\"Chunk ID: {chunk_id}\")\n",
    "    print(f\"{text_content}...\")\n",
    "    print(f\"Similarity Score: {score:.2f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above we can see the Chunk ID, their corresponding text and the calculated cosine similarity score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers (Generator Module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the Generator Module to generate answers based on the retrieved chunks using Azure OpenAI's GPT-4. In this step, we will create a function that takes a user query and the retrieved chunks, composes a relevant context from those chunks, and then uses GPT-4 to generate an answer based on this context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`model`**: Specifies the model, in our case GPT-4, for generating responses.\n",
    "- **`max_tokens`**: Limits response length.\n",
    "- **`temperature`**: Controls randomness; lower = focused, higher = creative.\n",
    "- **`top_p`**: Limits choices to most likely words for coherent output.\n",
    "- **`top_k`**: Restricts to top-k choices, narrowing token selection.\n",
    "- **`stop`**: Defines where the model should stop generating for clean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a highly knowledgeable AI assistant specializing in providing accurate and contextually relevant answers. \n",
    "    Use the context provided below to answer the user's query as thoroughly and concisely as possible. \n",
    "    If the context does not contain sufficient information to answer the query, say so explicitly.\n",
    "    Do not include any information not supported by the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, chunks, max_tokens=150, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Generate an answer to the given query using retrieved chunks and Azure OpenAI GPT-4.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        chunks (list): Retrieved chunks containing context.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    # Compose the context from the retrieved chunks, handling potential missing metadata\n",
    "    context = \" \".join(chunk.get('metadata', {}).get('text', '') for chunk in chunks)\n",
    "\n",
    "    # Ensure the context isn't empty\n",
    "    if not context.strip():\n",
    "        return \"The provided context does not contain sufficient information to answer the query.\"\n",
    "   \n",
    "    # Generate the answer using Azure OpenAI GPT-4\n",
    "    response = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        model = deployment_name,\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temperature,      # Lower temperature for concise and deterministic answers\n",
    "        stop = [\".\"],                   # Optional stop sequence for clean output\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Add period at the end of the answer if it doesn't exist\n",
    "    if answer and answer[-1] not in ['.', '?', '!']:\n",
    "        answer += \".\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " Qatar Petroleum (QP) mentioned that Phase 1 of the LNG expansion, also known as the North Field East project, will contribute to their carbon capture goal.\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 150\n",
    "temperature = 0.1\n",
    "\n",
    "# Generate answer based on retrieved chunks\n",
    "generated_answer = generate_answer(query, relevant_chunks, max_tokens, temperature)\n",
    "print(\"Generated Answer:\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " In Phase 1 of the LNG expansion, also known as the North Field East project, Qatar Petroleum (QP) plans to construct facilities that will increase Qatar's LNG capacity by 32 million tons per year.\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 150\n",
    "temperature = 0.9\n",
    "\n",
    "# Generate answer based on retrieved chunks\n",
    "generated_answer = generate_answer(query, relevant_chunks, max_tokens, temperature)\n",
    "print(\"Generated Answer:\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " In Phase 1 of the LNG expansion, also known as the North Field East project, Qatar Petroleum (QP) plans to raise its carbon capture storage from the current capacity to 7 million tons per year by 2027.\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "temperature = 0.9\n",
    "\n",
    "# Generate answer based on retrieved chunks\n",
    "generated_answer = generate_answer(query, relevant_chunks, max_tokens, temperature)\n",
    "print(\"Generated Answer:\\n\", generated_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Answer Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples provided, we can observe the following influences of the parameters `temperature` and `max_tokens` on the generated answers:\n",
    "\n",
    "1. **Influence of Temperature**:\n",
    "   - At a **lower temperature (0.1)**, the model generates concise and deterministic answers. These responses are focused, consistent and more factual.\n",
    "   \n",
    "   - At a **higher temperature (0.9)**, the answers become more creative and varied in phrasing, while still staying accurate to the context. This can be seen in the example above where there is stylistic differences in the responses (a bit more complex to read).\n",
    "\n",
    "2. **Influence of Max Tokens**:\n",
    "   - Increasing `max_tokens` to a high value (for example 1000) does not significantly change the answer length for our example questions. The model answers the question efficiently using fewer tokens.\n",
    "   \n",
    "   - A higher `max_tokens` ensures that longer contexts or more complex queries can be addressed without truncation, but for simple queries (like our example), the effect is minimal since the required answer fits well within the default limit (for example 150-200 tokens).\n",
    "\n",
    "These results demonstrate that while `temperature` significantly affects the tone and variability of responses, the `max_tokens` parameter acts as a safeguard to allow the model to generate longer answers when needed but does not force unnecessary verbosity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Contexts and Answers for Evaluation Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we utilize the evaluation dataset to retrieve relevant contexts and generate answers for each query. This allows us to prepare the necessary data for subsequent evaluation of retrieval and generation performance.\n",
    "\n",
    "We have `data_eval`, which contains the following fields `example_id`, `question_id`, `question`, `relevant_text`, `answer` and `article_url`. Each row represents an evaluation example, with the `question` to be queried in our pipeline, the `relevant_text` providing context for manual verification, and `answer` as the ground-truth answer to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevant_text</th>\n",
       "      <th>answer</th>\n",
       "      <th>article_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the innovation behind Leclanché's new ...</td>\n",
       "      <td>Leclanché said it has developed an environment...</td>\n",
       "      <td>Leclanché's innovation is using a water-based ...</td>\n",
       "      <td>https://www.sgvoice.net/strategy/technology/23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>The Green Deal Industrial Plan is a bid by the...</td>\n",
       "      <td>The EU’s Green Deal Industrial Plan aims to en...</td>\n",
       "      <td>https://www.sgvoice.net/policy/25396/eu-seeks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>The European counterpart to the US Inflation R...</td>\n",
       "      <td>The EU’s Green Deal Industrial Plan aims to en...</td>\n",
       "      <td>https://www.pv-magazine.com/2023/02/02/europea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>What are the four focus areas of the EU's Gree...</td>\n",
       "      <td>The new plan is fundamentally focused on four ...</td>\n",
       "      <td>The four focus areas of the EU's Green Deal In...</td>\n",
       "      <td>https://www.sgvoice.net/policy/25396/eu-seeks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>When did the cooperation between GM and Honda ...</td>\n",
       "      <td>What caught our eye was a new hookup between G...</td>\n",
       "      <td>July 2013</td>\n",
       "      <td>https://cleantechnica.com/2023/05/08/general-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id  question_id                                           question  \\\n",
       "0           1            1  What is the innovation behind Leclanché's new ...   \n",
       "1           2            2       What is the EU’s Green Deal Industrial Plan?   \n",
       "2           3            2       What is the EU’s Green Deal Industrial Plan?   \n",
       "3           4            3  What are the four focus areas of the EU's Gree...   \n",
       "4           5            4  When did the cooperation between GM and Honda ...   \n",
       "\n",
       "                                       relevant_text  \\\n",
       "0  Leclanché said it has developed an environment...   \n",
       "1  The Green Deal Industrial Plan is a bid by the...   \n",
       "2  The European counterpart to the US Inflation R...   \n",
       "3  The new plan is fundamentally focused on four ...   \n",
       "4  What caught our eye was a new hookup between G...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Leclanché's innovation is using a water-based ...   \n",
       "1  The EU’s Green Deal Industrial Plan aims to en...   \n",
       "2  The EU’s Green Deal Industrial Plan aims to en...   \n",
       "3  The four focus areas of the EU's Green Deal In...   \n",
       "4                                          July 2013   \n",
       "\n",
       "                                         article_url  \n",
       "0  https://www.sgvoice.net/strategy/technology/23...  \n",
       "1  https://www.sgvoice.net/policy/25396/eu-seeks-...  \n",
       "2  https://www.pv-magazine.com/2023/02/02/europea...  \n",
       "3  https://www.sgvoice.net/policy/25396/eu-seeks-...  \n",
       "4  https://cleantechnica.com/2023/05/08/general-m...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eval = pd.read_csv('../data/evaluation/cleantech_rag_evaluation_data_2024-09-20.csv', delimiter=';')\n",
    "data_eval.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom RAGPipeline class is instantiated with two core modules:\n",
    "\n",
    "- Retriever: Fetches relevant chunks using Pinecone based on the query embedding.\n",
    "- Generator: Generates an answer using the retrieved chunks and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query, embedding_model, top_k=5):\n",
    "        return self.retriever(query, embedding_model, top_k)\n",
    "\n",
    "    def generate_answer(self, query, retrieved_chunks):\n",
    "        return self.generator(query, retrieved_chunks)\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = RAGPipeline(\n",
    "    retriever=retrieve_relevant_chunks,\n",
    "    generator=generate_answer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLEU and ROUGE scores are calculated to measure textual overlap between the ground truth and the generated answer.\n",
    "- Cosine similarity between embeddings of the ground truth and generated answer is computed to assess semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_bleu_scores(reference, hypothesis):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    scores = {}\n",
    "    for n in range(1, 5):  # BLEU-1 to BLEU-4\n",
    "        weights = tuple((1 / n) for _ in range(n)) + (0,) * (4 - n)\n",
    "        scores[f\"bleu_{n}\"] = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            hypothesis.split(),\n",
    "            weights=weights,\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "def calculate_rouge_scores(reference, hypothesis):\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    rouge_scores = rouge.score(reference, hypothesis)\n",
    "    rouge1 = rouge_scores['rouge1'].fmeasure\n",
    "    rouge2 = rouge_scores['rouge2'].fmeasure\n",
    "    rougeL = rouge_scores['rougeL'].fmeasure\n",
    "    return {\"rouge1\": rouge1, \"rouge2\": rouge2, \"rougeL\": rougeL}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Contexts and Generating Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query in the evaluation dataset:\n",
    "\n",
    "- Retrieve the most relevant contexts using Pinecone.\n",
    "- Generate an answer based on the retrieved contexts using the generate_answer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating RAG Pipeline: 100%|██████████| 23/23 [04:05<00:00, 10.66s/it]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_example(query, true_answer, pipeline, embedding_model):\n",
    "    # Retrieve relevant chunks and generate an answer\n",
    "    relevant_chunks = pipeline.retrieve_relevant_chunks(query, embedding_model, top_k=5)\n",
    "    generated_answer = pipeline.generate_answer(query, relevant_chunks)\n",
    "\n",
    "    # Cosine Similarity\n",
    "    true_embedding = np.array(generate_embeddings(true_answer, embedding_model)).reshape(1, -1)\n",
    "    generated_embedding = np.array(generate_embeddings(generated_answer, embedding_model)).reshape(1, -1)\n",
    "    cosine_sim = cosine_similarity(true_embedding, generated_embedding)[0][0]\n",
    "\n",
    "    # BLEU and ROUGE Scores\n",
    "    bleu_scores = calculate_bleu_scores(true_answer, generated_answer)\n",
    "    rouge_scores = calculate_rouge_scores(true_answer, generated_answer)\n",
    "\n",
    "    # relevant chunks to list\n",
    "    relevant_chunks_list = [chunk.get('metadata', {}).get('text', '') for chunk in relevant_chunks['matches']]\n",
    "\n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"ground_truth\": true_answer,\n",
    "        \"answer\": generated_answer,\n",
    "        \"contexts\": relevant_chunks_list,\n",
    "        **bleu_scores,\n",
    "        **rouge_scores,\n",
    "        \"cosine_similarity\": cosine_sim\n",
    "    }\n",
    "\n",
    "def evaluate_pipeline(data_eval, pipeline, embedding_model):\n",
    "    results = []\n",
    "    for _, row in tqdm(data_eval.iterrows(), total=len(data_eval), desc=\"Retrieving Contexts and Generating Answers:\"):\n",
    "        query = row['question']\n",
    "        true_answer = row['answer']\n",
    "        \n",
    "        # Evaluate this example\n",
    "        example_results = evaluate_example(query, true_answer, pipeline, embedding_model)\n",
    "        results.append(example_results)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "evaluation_results = evaluate_pipeline(data_eval, pipeline, embedding_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have retrieved our top-k chunks and generated answers for each query, we can save this data and use it later in a different notebook to evaluate the performance of our RAG pipeline using the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to csv\n",
    "evaluation_results.to_csv('../data/recursive_2500_chunksize_300_overlap_text_embedding_3_large.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
