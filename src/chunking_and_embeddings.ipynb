{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate how to use the RAG pipeline to generate answers for the questions defined in the cleantech evaluation dataset. The pipeline consists of the following steps:\n",
    "1. Data Preparation and Ingestion\n",
    "2. Embedding\n",
    "3. Store Embeddings in Vector Database\n",
    "4. Retrieve Contexts\n",
    "5. Generate Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "\n",
    "import credentials\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name='gpt-4' #This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Ingestion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load and prepare the data, create embeddings for each chunk, and store the embeddings in a vector database. Since the data is already cleaned, we'll move directly to chunking and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qatar to Slash Emissions as LNG Expansion Adva...</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>Qatar Petroleum (QP) is targeting aggressive c...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India Launches Its First 700 MW PHWR</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>Nuclear Power Corp of India Ltd (NPCIL) synchr...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Chapter for US-China Energy Trade</td>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>New US President Joe Biden took office this we...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japan: Slow Restarts Cast Doubt on 2030 Energy...</td>\n",
       "      <td>2021-01-22</td>\n",
       "      <td>The slow pace of Japanese reactor restarts con...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYC Pension Funds to Divest Fossil Fuel Shares</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>Two of New York City's largest pension funds s...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date  \\\n",
       "0  Qatar to Slash Emissions as LNG Expansion Adva...  2021-01-13   \n",
       "1               India Launches Its First 700 MW PHWR  2021-01-15   \n",
       "2              New Chapter for US-China Energy Trade  2021-01-20   \n",
       "3  Japan: Slow Restarts Cast Doubt on 2030 Energy...  2021-01-22   \n",
       "4     NYC Pension Funds to Divest Fossil Fuel Shares  2021-01-25   \n",
       "\n",
       "                                             content       domain  \\\n",
       "0  Qatar Petroleum (QP) is targeting aggressive c...  energyintel   \n",
       "1  Nuclear Power Corp of India Ltd (NPCIL) synchr...  energyintel   \n",
       "2  New US President Joe Biden took office this we...  energyintel   \n",
       "3  The slow pace of Japanese reactor restarts con...  energyintel   \n",
       "4  Two of New York City's largest pension funds s...  energyintel   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.energyintel.com/0000017b-a7dc-de4c...  \n",
       "1  https://www.energyintel.com/0000017b-a7dc-de4c...  \n",
       "2  https://www.energyintel.com/0000017b-a7dc-de4c...  \n",
       "3  https://www.energyintel.com/0000017b-a7dc-de4c...  \n",
       "4  https://www.energyintel.com/0000017b-a7dc-de4c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/processed/cleantech_processed.csv')\n",
    "data_eval = pd.read_csv('../data/evaluation/cleantech_rag_evaluation_data_2024-09-20.csv', delimiter=\";\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in the dataset:  9593\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of articles in the dataset: \", len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `chunk_size`: Sets the maximum character length of each chunk to 1000 characters.\n",
    "- `chunk_overlap`: Adds a n-character overlap between chunks to preserve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>content_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qatar to Slash Emissions as LNG Expansion Adva...</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>Qatar Petroleum (QP) is targeting aggressive c...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "      <td>Qatar Petroleum (QP) is targeting aggressive c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qatar to Slash Emissions as LNG Expansion Adva...</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>Qatar Petroleum (QP) is targeting aggressive c...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "      <td>eyeing a phased expansion to 126 million tons/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qatar to Slash Emissions as LNG Expansion Adva...</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>Qatar Petroleum (QP) is targeting aggressive c...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "      <td>estimates to be worth around $ 35 billion, is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India Launches Its First 700 MW PHWR</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>Nuclear Power Corp of India Ltd (NPCIL) synchr...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "      <td>Nuclear Power Corp of India Ltd (NPCIL) synchr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India Launches Its First 700 MW PHWR</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>Nuclear Power Corp of India Ltd (NPCIL) synchr...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>https://www.energyintel.com/0000017b-a7dc-de4c...</td>\n",
       "      <td>start up, although order flows will depend on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date  \\\n",
       "0  Qatar to Slash Emissions as LNG Expansion Adva...  2021-01-13   \n",
       "1  Qatar to Slash Emissions as LNG Expansion Adva...  2021-01-13   \n",
       "2  Qatar to Slash Emissions as LNG Expansion Adva...  2021-01-13   \n",
       "3               India Launches Its First 700 MW PHWR  2021-01-15   \n",
       "4               India Launches Its First 700 MW PHWR  2021-01-15   \n",
       "\n",
       "                                             content       domain  \\\n",
       "0  Qatar Petroleum (QP) is targeting aggressive c...  energyintel   \n",
       "1  Qatar Petroleum (QP) is targeting aggressive c...  energyintel   \n",
       "2  Qatar Petroleum (QP) is targeting aggressive c...  energyintel   \n",
       "3  Nuclear Power Corp of India Ltd (NPCIL) synchr...  energyintel   \n",
       "4  Nuclear Power Corp of India Ltd (NPCIL) synchr...  energyintel   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.energyintel.com/0000017b-a7dc-de4c...   \n",
       "1  https://www.energyintel.com/0000017b-a7dc-de4c...   \n",
       "2  https://www.energyintel.com/0000017b-a7dc-de4c...   \n",
       "3  https://www.energyintel.com/0000017b-a7dc-de4c...   \n",
       "4  https://www.energyintel.com/0000017b-a7dc-de4c...   \n",
       "\n",
       "                                      content_chunks  \n",
       "0  Qatar Petroleum (QP) is targeting aggressive c...  \n",
       "1  eyeing a phased expansion to 126 million tons/...  \n",
       "2  estimates to be worth around $ 35 billion, is ...  \n",
       "3  Nuclear Power Corp of India Ltd (NPCIL) synchr...  \n",
       "4  start up, although order flows will depend on ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_text(dataframe, text_column, chunk_size=1000, chunk_overlap=100):\n",
    "    # Initialize RecursiveCharacterTextSplitter with dynamic parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    # Split text in the specified column\n",
    "    dataframe['content_chunks'] = dataframe[text_column].apply(lambda text: text_splitter.split_text(text))\n",
    "    \n",
    "    # Flatten the DataFrame for individual chunk rows\n",
    "    chunked_df = dataframe.explode('content_chunks').reset_index(drop=True)\n",
    "\n",
    "    return chunked_df\n",
    "\n",
    "# Apply the chunking with adjustable parameters\n",
    "chunked_data = chunk_text(data, text_column='content', chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunked_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44795"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chunked the texts into smaller segments, the next step is to pass these chunks through an embedding model to obtain their vector representations. The embedding model maps the textual information into high-dimensional vector spaces, where semantic similarities and relationships are preserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings in Parallel:  57%|█████▋    | 509/896 [1:07:10<35:21,  5.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating embeddings for batch: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Embeddings_Create Operation under Azure OpenAI API version 2023-12-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Error generating embeddings for batch: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Embeddings_Create Operation under Azure OpenAI API version 2023-12-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings in Parallel:  64%|██████▍   | 573/896 [1:15:14<58:03, 10.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating embeddings for batch: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Embeddings_Create Operation under Azure OpenAI API version 2023-12-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Error generating embeddings for batch: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Embeddings_Create Operation under Azure OpenAI API version 2023-12-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings in Parallel:  97%|█████████▋| 866/896 [1:52:44<02:22,  4.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating embeddings for batch: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Embeddings_Create Operation under Azure OpenAI API version 2023-12-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings in Parallel: 100%|██████████| 896/896 [1:56:46<00:00,  7.82s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "MAX_WORKERS = 8  # Number of threads\n",
    "RETRY_DELAY = 60  # Retry delay in seconds for rate-limiting errors\n",
    "\n",
    "# Function to generate embeddings for a batch of texts with retry logic\n",
    "def embed_batch(text_batch, model, retries=3):\n",
    "    attempts = 0\n",
    "    while attempts < retries:\n",
    "        try:\n",
    "            response = client.embeddings.create(input=text_batch, model=model)\n",
    "            return [item.embedding for item in response.data]\n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                print(f\"Rate limit exceeded. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)  # Wait before retrying\n",
    "                attempts += 1\n",
    "            else:\n",
    "                print(f\"Error embedding batch: {e}\")\n",
    "                return [None] * len(text_batch)\n",
    "    print(\"Failed to embed after multiple attempts.\")\n",
    "    return [None] * len(text_batch)\n",
    "\n",
    "# Function to process batches in parallel with rate limiting\n",
    "def embed_in_batches(data, model=EMBEDDING_MODEL, batch_size=BATCH_SIZE, max_workers=MAX_WORKERS):\n",
    "    total_chunks = len(data)\n",
    "    num_batches = math.ceil(total_chunks / batch_size)\n",
    "    \n",
    "    # Create batches\n",
    "    batches = [data[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
    "    embeddings = [None] * total_chunks  # Placeholder for embeddings\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        for i, batch in enumerate(batches):\n",
    "            futures[executor.submit(embed_batch, batch, model)] = i\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Embedding in Parallel\"):\n",
    "            batch_index = futures[future]\n",
    "            try:\n",
    "                batch_embeddings = future.result()\n",
    "                embeddings[batch_index * batch_size: (batch_index + 1) * batch_size] = batch_embeddings\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_index}: {e}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Prepare data for embedding generation\n",
    "texts_to_embed = chunked_data[\"content_chunks\"].tolist()\n",
    "\n",
    "# Run batch embedding generation with rate limiting\n",
    "chunked_data['embeddings'] = embed_in_batches(\n",
    "    texts_to_embed, \n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_workers=MAX_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the embeddings:  1536\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the embeddings: \", len(chunked_data['embeddings'][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chunked_data` DataFrame is structured so that each row represents an individual **chunk** of text derived from the original documents, with a corresponding **embedding vector** stored in the `embeddings` column. Each embedding vector is a dense list of floating-point numbers (1536 dimensions in our case, given the use of the `text-embedding-ada-002` model) that encapsulates the semantic meaning of the text chunk.\n",
    "\n",
    "These embeddings are critical for the RAG system, as they allow efficient similarity searches and retrieval tasks by representing the content of each text chunk in a high-dimensional vector space. When a query is embedded, the RAG system can quickly locate the most relevant text chunks by identifying embedding vectors in the database that are closest in meaning. This approach enables the system to retrieve contextually relevant information by comparing semantic relationships, streamlining the entire retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save embeddings and corresponding chunked data to a file\n",
    "with open('../recursive_{CHUNK_SIZE}_chunksize_{CHUNK_OVERLAP}_overlap_{EMBEDDING_MODEL}.pkl', 'wb') as f:\n",
    "    pickle.dump(chunked_data, f)\n",
    "\n",
    "print(\"Embeddings and chunked data saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd9c78cbb62892232a2e8cf9a4bd699d988202e949c50bb9be5232199c394801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
